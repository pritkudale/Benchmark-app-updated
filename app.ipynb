{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d510c4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MMLU-Pro</th>\n",
       "      <th>GPQA Diamond</th>\n",
       "      <th>Humanity's Last Exam</th>\n",
       "      <th>LiveCodeBench</th>\n",
       "      <th>SciCode</th>\n",
       "      <th>HumanEval</th>\n",
       "      <th>MATH-500</th>\n",
       "      <th>AIME 2024</th>\n",
       "      <th>Multilingual Index</th>\n",
       "      <th>Input Cost</th>\n",
       "      <th>Cached Input Cost</th>\n",
       "      <th>Output Cost</th>\n",
       "      <th>Provider</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OpenAI o1</td>\n",
       "      <td>84.1</td>\n",
       "      <td>74.7</td>\n",
       "      <td>7.7</td>\n",
       "      <td>67.9</td>\n",
       "      <td>35.8</td>\n",
       "      <td>97.4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>72.3</td>\n",
       "      <td>87.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Azure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Claude 3.5 Sonnet</td>\n",
       "      <td>77.2</td>\n",
       "      <td>59.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>38.1</td>\n",
       "      <td>35.1</td>\n",
       "      <td>93.0</td>\n",
       "      <td>77.1</td>\n",
       "      <td>15.7</td>\n",
       "      <td>88.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>15.0</td>\n",
       "      <td>AWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GPT-4o</td>\n",
       "      <td>74.8</td>\n",
       "      <td>54.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>30.9</td>\n",
       "      <td>33.3</td>\n",
       "      <td>93.0</td>\n",
       "      <td>75.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>83.8</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.25</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Azure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama 3.1 405b (AWS)</td>\n",
       "      <td>73.2</td>\n",
       "      <td>51.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>30.5</td>\n",
       "      <td>29.9</td>\n",
       "      <td>85.4</td>\n",
       "      <td>70.3</td>\n",
       "      <td>21.3</td>\n",
       "      <td>76.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.4</td>\n",
       "      <td>AWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Llama 3.1 405b (GCP)</td>\n",
       "      <td>73.2</td>\n",
       "      <td>51.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>30.5</td>\n",
       "      <td>29.9</td>\n",
       "      <td>85.4</td>\n",
       "      <td>70.3</td>\n",
       "      <td>21.3</td>\n",
       "      <td>76.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>GCP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  MMLU-Pro  GPQA Diamond  Humanity's Last Exam  \\\n",
       "0             OpenAI o1      84.1          74.7                   7.7   \n",
       "1     Claude 3.5 Sonnet      77.2          59.9                   3.9   \n",
       "2                GPT-4o      74.8          54.3                   3.3   \n",
       "3  Llama 3.1 405b (AWS)      73.2          51.5                   4.2   \n",
       "4  Llama 3.1 405b (GCP)      73.2          51.5                   4.2   \n",
       "\n",
       "   LiveCodeBench  SciCode  HumanEval  MATH-500  AIME 2024  Multilingual Index  \\\n",
       "0           67.9     35.8       97.4      97.0       72.3                87.6   \n",
       "1           38.1     35.1       93.0      77.1       15.7                88.4   \n",
       "2           30.9     33.3       93.0      75.9       15.0                83.8   \n",
       "3           30.5     29.9       85.4      70.3       21.3                76.5   \n",
       "4           30.5     29.9       85.4      70.3       21.3                76.5   \n",
       "\n",
       "   Input Cost  Cached Input Cost  Output Cost Provider  \n",
       "0        15.0               7.50         60.0    Azure  \n",
       "1         3.0               3.75         15.0      AWS  \n",
       "2         2.5               1.25         10.0    Azure  \n",
       "3         2.4                NaN          2.4      AWS  \n",
       "4         5.0                NaN         16.0      GCP  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file back into a DataFrame\n",
    "df = pd.read_csv(\"ai_model_benchmarks.csv\")\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e5be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aca6e0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Model': 'OpenAI o1', 'MMLU-Pro': 84.1, 'GPQA Diamond': 74.7, \"Humanity's Last Exam\": 7.7, 'LiveCodeBench': 67.9, 'SciCode': 35.8, 'HumanEval': 97.4, 'MATH-500': 97.0, 'AIME 2024': 72.3, 'Multilingual Index': 87.6, 'Input Cost': 15.0, 'Cached Input Cost': 7.5, 'Output Cost': 60.0, 'Provider': 'Azure'}, {'Model': 'Claude 3.5 Sonnet', 'MMLU-Pro': 77.2, 'GPQA Diamond': 59.9, \"Humanity's Last Exam\": 3.9, 'LiveCodeBench': 38.1, 'SciCode': 35.1, 'HumanEval': 93.0, 'MATH-500': 77.1, 'AIME 2024': 15.7, 'Multilingual Index': 88.4, 'Input Cost': 3.0, 'Cached Input Cost': 3.75, 'Output Cost': 15.0, 'Provider': 'AWS'}, {'Model': 'GPT-4o', 'MMLU-Pro': 74.8, 'GPQA Diamond': 54.3, \"Humanity's Last Exam\": 3.3, 'LiveCodeBench': 30.9, 'SciCode': 33.3, 'HumanEval': 93.0, 'MATH-500': 75.9, 'AIME 2024': 15.0, 'Multilingual Index': 83.8, 'Input Cost': 2.5, 'Cached Input Cost': 1.25, 'Output Cost': 10.0, 'Provider': 'Azure'}, {'Model': 'Llama 3.1 405b (AWS)', 'MMLU-Pro': 73.2, 'GPQA Diamond': 51.5, \"Humanity's Last Exam\": 4.2, 'LiveCodeBench': 30.5, 'SciCode': 29.9, 'HumanEval': 85.4, 'MATH-500': 70.3, 'AIME 2024': 21.3, 'Multilingual Index': 76.5, 'Input Cost': 2.4, 'Cached Input Cost': nan, 'Output Cost': 2.4, 'Provider': 'AWS'}, {'Model': 'Llama 3.1 405b (GCP)', 'MMLU-Pro': 73.2, 'GPQA Diamond': 51.5, \"Humanity's Last Exam\": 4.2, 'LiveCodeBench': 30.5, 'SciCode': 29.9, 'HumanEval': 85.4, 'MATH-500': 70.3, 'AIME 2024': 21.3, 'Multilingual Index': 76.5, 'Input Cost': 5.0, 'Cached Input Cost': nan, 'Output Cost': 16.0, 'Provider': 'GCP'}, {'Model': 'OpenAI o1-mini', 'MMLU-Pro': 74.2, 'GPQA Diamond': 60.3, \"Humanity's Last Exam\": 4.9, 'LiveCodeBench': 57.6, 'SciCode': 32.3, 'HumanEval': 97.2, 'MATH-500': 94.4, 'AIME 2024': 60.3, 'Multilingual Index': 83.3, 'Input Cost': 1.1, 'Cached Input Cost': 0.55, 'Output Cost': 4.4, 'Provider': 'Azure'}, {'Model': 'GPT-4 Turbo', 'MMLU-Pro': 69.4, 'GPQA Diamond': nan, \"Humanity's Last Exam\": 3.3, 'LiveCodeBench': 29.1, 'SciCode': 29.5, 'HumanEval': 91.8, 'MATH-500': 73.7, 'AIME 2024': 9.7, 'Multilingual Index': nan, 'Input Cost': 10.0, 'Cached Input Cost': nan, 'Output Cost': 30.0, 'Provider': 'Azure'}, {'Model': 'Claude 3 Opus', 'MMLU-Pro': 69.6, 'GPQA Diamond': 48.9, \"Humanity's Last Exam\": 3.1, 'LiveCodeBench': 27.9, 'SciCode': 23.3, 'HumanEval': 84.8, 'MATH-500': 64.1, 'AIME 2024': 3.3, 'Multilingual Index': nan, 'Input Cost': 15.0, 'Cached Input Cost': nan, 'Output Cost': 75.0, 'Provider': 'AWS'}, {'Model': 'DeepSeek V3', 'MMLU-Pro': 75.7, 'GPQA Diamond': 55.7, \"Humanity's Last Exam\": 3.6, 'LiveCodeBench': 35.9, 'SciCode': 35.4, 'HumanEval': 90.6, 'MATH-500': 88.7, 'AIME 2024': 25.3, 'Multilingual Index': 86.4, 'Input Cost': nan, 'Cached Input Cost': nan, 'Output Cost': nan, 'Provider': nan}, {'Model': 'GPT-4', 'MMLU-Pro': nan, 'GPQA Diamond': nan, \"Humanity's Last Exam\": nan, 'LiveCodeBench': nan, 'SciCode': nan, 'HumanEval': 88.4, 'MATH-500': nan, 'AIME 2024': nan, 'Multilingual Index': nan, 'Input Cost': 30.0, 'Cached Input Cost': nan, 'Output Cost': 60.0, 'Provider': 'Azure'}, {'Model': 'Llama 3.1 70b', 'MMLU-Pro': 67.6, 'GPQA Diamond': 41.0, \"Humanity's Last Exam\": 4.6, 'LiveCodeBench': 23.2, 'SciCode': 26.7, 'HumanEval': 81.2, 'MATH-500': 64.9, 'AIME 2024': 17.3, 'Multilingual Index': nan, 'Input Cost': 0.72, 'Cached Input Cost': nan, 'Output Cost': 0.72, 'Provider': 'AWS'}, {'Model': 'Llama 3.3 70b', 'MMLU-Pro': 71.3, 'GPQA Diamond': 50.0, \"Humanity's Last Exam\": 4.0, 'LiveCodeBench': 28.8, 'SciCode': 26.0, 'HumanEval': 86.0, 'MATH-500': 77.3, 'AIME 2024': 30.0, 'Multilingual Index': 83.9, 'Input Cost': 0.72, 'Cached Input Cost': nan, 'Output Cost': 0.72, 'Provider': 'AWS'}, {'Model': 'Gemini 1.5 Pro', 'MMLU-Pro': 75.0, 'GPQA Diamond': 58.9, \"Humanity's Last Exam\": 4.9, 'LiveCodeBench': 31.6, 'SciCode': 29.5, 'HumanEval': 89.8, 'MATH-500': 87.6, 'AIME 2024': 23.0, 'Multilingual Index': 85.0, 'Input Cost': 5.0, 'Cached Input Cost': nan, 'Output Cost': 10.0, 'Provider': 'GCP'}, {'Model': 'Claude 3.5 Haiku', 'MMLU-Pro': 63.4, 'GPQA Diamond': 41.0, \"Humanity's Last Exam\": 3.5, 'LiveCodeBench': 31.4, 'SciCode': 26.0, 'HumanEval': 85.9, 'MATH-500': 72.1, 'AIME 2024': 3.3, 'Multilingual Index': 78.5, 'Input Cost': 0.8, 'Cached Input Cost': 1.0, 'Output Cost': 4.0, 'Provider': 'AWS'}, {'Model': 'Gemini 1.5 Flash', 'MMLU-Pro': 67.8, 'GPQA Diamond': 46.3, \"Humanity's Last Exam\": 3.5, 'LiveCodeBench': 27.3, 'SciCode': nan, 'HumanEval': 83.8, 'MATH-500': 82.7, 'AIME 2024': 18.0, 'Multilingual Index': 80.7, 'Input Cost': 0.3, 'Cached Input Cost': nan, 'Output Cost': 0.6, 'Provider': 'GCP'}, {'Model': 'Claude 3 Haiku', 'MMLU-Pro': nan, 'GPQA Diamond': nan, \"Humanity's Last Exam\": nan, 'LiveCodeBench': 16.2, 'SciCode': 17.7, 'HumanEval': 70.6, 'MATH-500': 39.4, 'AIME 2024': nan, 'Multilingual Index': 68.3, 'Input Cost': 0.25, 'Cached Input Cost': nan, 'Output Cost': 1.25, 'Provider': 'AWS'}, {'Model': 'Llama 3.1 8b', 'MMLU-Pro': 47.6, 'GPQA Diamond': 26.0, \"Humanity's Last Exam\": 5.1, 'LiveCodeBench': 11.6, 'SciCode': 13.2, 'HumanEval': 66.5, 'MATH-500': 51.9, 'AIME 2024': 7.7, 'Multilingual Index': 61.0, 'Input Cost': 0.22, 'Cached Input Cost': nan, 'Output Cost': 0.22, 'Provider': 'AWS'}, {'Model': 'GPT-3.5 Turbo', 'MMLU-Pro': nan, 'GPQA Diamond': nan, \"Humanity's Last Exam\": nan, 'LiveCodeBench': nan, 'SciCode': nan, 'HumanEval': nan, 'MATH-500': nan, 'AIME 2024': nan, 'Multilingual Index': nan, 'Input Cost': 0.5, 'Cached Input Cost': nan, 'Output Cost': 1.5, 'Provider': 'Azure'}, {'Model': 'Gemini 2.0 Flash', 'MMLU-Pro': 77.9, 'GPQA Diamond': 62.3, \"Humanity's Last Exam\": 5.3, 'LiveCodeBench': 33.4, 'SciCode': 31.2, 'HumanEval': 90.4, 'MATH-500': 93.0, 'AIME 2024': 33.0, 'Multilingual Index': nan, 'Input Cost': 0.15, 'Cached Input Cost': nan, 'Output Cost': 0.6, 'Provider': 'GCP'}, {'Model': 'AWS Nova Micro', 'MMLU-Pro': 53.1, 'GPQA Diamond': 35.8, \"Humanity's Last Exam\": 3.4, 'LiveCodeBench': 14.0, 'SciCode': 9.4, 'HumanEval': 79.9, 'MATH-500': 70.3, 'AIME 2024': 8.0, 'Multilingual Index': 71.1, 'Input Cost': 0.035, 'Cached Input Cost': nan, 'Output Cost': 0.14, 'Provider': 'AWS'}, {'Model': 'AWS Nova Lite', 'MMLU-Pro': 59.0, 'GPQA Diamond': 43.3, \"Humanity's Last Exam\": 4.6, 'LiveCodeBench': 16.7, 'SciCode': 13.8, 'HumanEval': 82.8, 'MATH-500': 76.5, 'AIME 2024': 10.7, 'Multilingual Index': 76.1, 'Input Cost': 0.06, 'Cached Input Cost': nan, 'Output Cost': 0.24, 'Provider': 'AWS'}, {'Model': 'AWS Nova Pro', 'MMLU-Pro': 69.1, 'GPQA Diamond': 49.9, \"Humanity's Last Exam\": 4.7, 'LiveCodeBench': 23.3, 'SciCode': 20.8, 'HumanEval': 84.1, 'MATH-500': 78.6, 'AIME 2024': 10.7, 'Multilingual Index': 83.4, 'Input Cost': 0.8, 'Cached Input Cost': nan, 'Output Cost': 3.2, 'Provider': 'AWS'}, {'Model': 'GPT-4o mini', 'MMLU-Pro': 64.8, 'GPQA Diamond': 43.0, \"Humanity's Last Exam\": 4.0, 'LiveCodeBench': 23.4, 'SciCode': 22.9, 'HumanEval': 87.6, 'MATH-500': 78.9, 'AIME 2024': 11.7, 'Multilingual Index': 80.5, 'Input Cost': 0.15, 'Cached Input Cost': 0.075, 'Output Cost': 0.6, 'Provider': 'Azure'}, {'Model': 'OpenAI o3-mini', 'MMLU-Pro': 79.1, 'GPQA Diamond': 74.8, \"Humanity's Last Exam\": 8.7, 'LiveCodeBench': 71.7, 'SciCode': 39.8, 'HumanEval': 97.2, 'MATH-500': 97.3, 'AIME 2024': 77.0, 'Multilingual Index': nan, 'Input Cost': 1.1, 'Cached Input Cost': 0.55, 'Output Cost': 4.4, 'Provider': 'Azure'}, {'Model': 'OpenAI o3-mini High', 'MMLU-Pro': 80.2, 'GPQA Diamond': 77.3, \"Humanity's Last Exam\": 12.3, 'LiveCodeBench': 73.4, 'SciCode': 39.9, 'HumanEval': nan, 'MATH-500': 98.5, 'AIME 2024': 86.0, 'Multilingual Index': nan, 'Input Cost': 1.1, 'Cached Input Cost': 0.55, 'Output Cost': 4.4, 'Provider': 'Azure'}, {'Model': 'DeepSeek-R1', 'MMLU-Pro': 84.4, 'GPQA Diamond': 70.8, \"Humanity's Last Exam\": 9.3, 'LiveCodeBench': 61.7, 'SciCode': 35.7, 'HumanEval': 97.7, 'MATH-500': 96.3, 'AIME 2024': 68.3, 'Multilingual Index': nan, 'Input Cost': 1.35, 'Cached Input Cost': nan, 'Output Cost': 5.4, 'Provider': 'AWS'}, {'Model': 'GPT-4.5', 'MMLU-Pro': nan, 'GPQA Diamond': 71.4, \"Humanity's Last Exam\": nan, 'LiveCodeBench': nan, 'SciCode': nan, 'HumanEval': nan, 'MATH-500': nan, 'AIME 2024': 36.7, 'Multilingual Index': nan, 'Input Cost': 75.0, 'Cached Input Cost': 37.5, 'Output Cost': 150.0, 'Provider': 'Azure'}, {'Model': 'Claude 3.7 Sonnet', 'MMLU-Pro': 80.3, 'GPQA Diamond': 65.6, \"Humanity's Last Exam\": 4.8, 'LiveCodeBench': 39.4, 'SciCode': 37.5, 'HumanEval': 92.2, 'MATH-500': 83.5, 'AIME 2024': 24.3, 'Multilingual Index': nan, 'Input Cost': 3.0, 'Cached Input Cost': 3.75, 'Output Cost': 15.0, 'Provider': 'AWS'}, {'Model': 'Gemini 2.0 Flash Lite', 'MMLU-Pro': 72.3, 'GPQA Diamond': 54.2, \"Humanity's Last Exam\": 4.4, 'LiveCodeBench': 17.9, 'SciCode': 27.7, 'HumanEval': 89.6, 'MATH-500': 87.3, 'AIME 2024': 30.3, 'Multilingual Index': nan, 'Input Cost': 0.075, 'Cached Input Cost': nan, 'Output Cost': 0.3, 'Provider': 'GCP'}, {'Model': 'GPT-4.1', 'MMLU-Pro': 80.6, 'GPQA Diamond': 66.6, \"Humanity's Last Exam\": 4.6, 'LiveCodeBench': 45.7, 'SciCode': 38.1, 'HumanEval': 95.6, 'MATH-500': 91.3, 'AIME 2024': 43.7, 'Multilingual Index': nan, 'Input Cost': 2.0, 'Cached Input Cost': 0.5, 'Output Cost': 8.0, 'Provider': 'Azure'}, {'Model': 'GPT-4.1 mini', 'MMLU-Pro': 78.1, 'GPQA Diamond': 66.4, \"Humanity's Last Exam\": 4.6, 'LiveCodeBench': 48.3, 'SciCode': 40.4, 'HumanEval': 95.0, 'MATH-500': 92.5, 'AIME 2024': 43.0, 'Multilingual Index': nan, 'Input Cost': 0.4, 'Cached Input Cost': 0.1, 'Output Cost': 1.6, 'Provider': 'Azure'}, {'Model': 'GPT-4.1 nano', 'MMLU-Pro': 65.7, 'GPQA Diamond': 51.2, \"Humanity's Last Exam\": 3.9, 'LiveCodeBench': 32.6, 'SciCode': 25.9, 'HumanEval': 87.7, 'MATH-500': 84.8, 'AIME 2024': 23.7, 'Multilingual Index': nan, 'Input Cost': 0.1, 'Cached Input Cost': 0.03, 'Output Cost': 0.4, 'Provider': 'Azure'}, {'Model': 'Llama 4 Maverick', 'MMLU-Pro': 80.9, 'GPQA Diamond': 67.1, \"Humanity's Last Exam\": 4.8, 'LiveCodeBench': 39.7, 'SciCode': 33.1, 'HumanEval': 87.9, 'MATH-500': 88.9, 'AIME 2024': 39.0, 'Multilingual Index': nan, 'Input Cost': 0.2, 'Cached Input Cost': nan, 'Output Cost': 0.6, 'Provider': 'GCP'}, {'Model': 'Llama 4 Scout', 'MMLU-Pro': 75.2, 'GPQA Diamond': 58.7, \"Humanity's Last Exam\": 4.3, 'LiveCodeBench': 29.9, 'SciCode': 17.0, 'HumanEval': 82.6, 'MATH-500': 84.4, 'AIME 2024': 28.3, 'Multilingual Index': nan, 'Input Cost': 0.15, 'Cached Input Cost': nan, 'Output Cost': 0.6, 'Provider': 'GCP'}, {'Model': 'o4-mini (high)', 'MMLU-Pro': 83.2, 'GPQA Diamond': 78.4, \"Humanity's Last Exam\": 17.5, 'LiveCodeBench': 80.4, 'SciCode': 46.5, 'HumanEval': 99.0, 'MATH-500': 98.9, 'AIME 2024': 94.0, 'Multilingual Index': nan, 'Input Cost': 1.1, 'Cached Input Cost': 0.275, 'Output Cost': 4.4, 'Provider': 'Azure'}, {'Model': 'Gemini 2.5 Flash', 'MMLU-Pro': 80.2, 'GPQA Diamond': 69.5, \"Humanity's Last Exam\": nan, 'LiveCodeBench': nan, 'SciCode': 21.5, 'HumanEval': nan, 'MATH-500': nan, 'AIME 2024': 43.3, 'Multilingual Index': nan, 'Input Cost': 0.15, 'Cached Input Cost': nan, 'Output Cost': 0.6, 'Provider': 'GCP'}, {'Model': 'Gemini 2.5 Flash Thinking', 'MMLU-Pro': 80.0, 'GPQA Diamond': 69.8, \"Humanity's Last Exam\": 11.6, 'LiveCodeBench': 50.5, 'SciCode': 35.9, 'HumanEval': nan, 'MATH-500': 98.1, 'AIME 2024': 84.3, 'Multilingual Index': nan, 'Input Cost': 0.15, 'Cached Input Cost': nan, 'Output Cost': 3.5, 'Provider': 'GCP'}, {'Model': 'Gemini 2.5 Pro', 'MMLU-Pro': 85.8, 'GPQA Diamond': 83.6, \"Humanity's Last Exam\": 17.1, 'LiveCodeBench': 69.5, 'SciCode': 39.5, 'HumanEval': 98.5, 'MATH-500': 98.0, 'AIME 2024': 87.0, 'Multilingual Index': nan, 'Input Cost': 1.25, 'Cached Input Cost': 0.31, 'Output Cost': 10.0, 'Provider': 'GCP'}]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
